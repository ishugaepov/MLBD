{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CTR-prediction\" data-toc-modified-id=\"CTR-prediction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CTR-prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Formulation\" data-toc-modified-id=\"Problem-Formulation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Formulation</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-construction:\" data-toc-modified-id=\"Dataset-construction:-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Dataset construction:</a></span></li><li><span><a href=\"#Format:\" data-toc-modified-id=\"Format:-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Format:</a></span></li></ul></li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Metrics</a></span></li></ul></li><li><span><a href=\"#Dataset-preprocessing\" data-toc-modified-id=\"Dataset-preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#ML-Pipelines-(Transformers,-Estimators)\" data-toc-modified-id=\"ML-Pipelines-(Transformers,-Estimators)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components\" target=\"_blank\">ML Pipelines (Transformers, Estimators)</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-stages-of-pipeline\" data-toc-modified-id=\"Prepare-stages-of-pipeline-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Prepare stages of pipeline</a></span></li><li><span><a href=\"#Fit-and-save-pipeline\" data-toc-modified-id=\"Fit-and-save-pipeline-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Fit and save pipeline</a></span></li><li><span><a href=\"#Load-fitted-pipeline\" data-toc-modified-id=\"Load-fitted-pipeline-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Load fitted pipeline</a></span></li><li><span><a href=\"#Transform-dataset-using-pipeline\" data-toc-modified-id=\"Transform-dataset-using-pipeline-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Transform dataset using pipeline</a></span></li><li><span><a href=\"#Make-dataset-split\" data-toc-modified-id=\"Make-dataset-split-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Make dataset split</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html\" target=\"_blank\">Classification</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\" target=\"_blank\">Logistic Regression</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-and-Train-model\" data-toc-modified-id=\"Define-and-Train-model-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Define and Train model</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html\" target=\"_blank\">Evaluation</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-classification-metrics\" data-toc-modified-id=\"Binary-classification-metrics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification\" target=\"_blank\">Binary classification metrics</a></a></span></li><li><span><a href=\"#Make-submission\" data-toc-modified-id=\"Make-submission-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Make submission</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR-prediction\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "$\\newcommand{\\vecw}{{\\bf w}}$\n",
    "$\\newcommand{\\vecx}{{\\bf x}}$\n",
    "\n",
    "* Dataset: $X^N = \\{ z_i \\}^N_{i=1}$, где $z_i = (\\vecx_i, y_i) \\sim P(z), y_i \\in \\{0,1\\}$\n",
    "* Prediction: $$ \\hat{y}_i = f_{\\vecw}(\\vecx_i) =  \\mathbb{P} \\left\\{ y = 1 \\mid \\vecx_i \\right\\} $$\n",
    "* Loss function (Binary Cross-Entropy): $$ \\min\\limits_{\\vecw} \\quad \\frac{\\lambda}{2}\\| \\vecw \\|^2_2 - \\frac{1}{N} \\sum\\limits_{i=1}^{N} y_i \\log \\hat{y}_i + (1-y_i) \\log(1-\\hat{y}_i) $$\n",
    "\n",
    "## Dataset\n",
    "$ $\n",
    "<details>\n",
    "  <summary>Click here to see the details</summary>\n",
    "\n",
    "For more details see `/data/criteo/readme.txt`\n",
    "\n",
    "### Dataset construction:\n",
    "\n",
    "\n",
    ">There are 13 features taking **integer** values and 26\n",
    "**categorical** features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes. \n",
    "Some features may have missing values.\n",
    "\n",
    "> The rows are chronologically ordered by `id` column.\n",
    "\n",
    "> The test set corresponds to events on the day following the training period. \n",
    "The first column (`label`) has been removed.\n",
    "\n",
    "\n",
    "### Format:\n",
    "\n",
    "> The columns are comma separeted with the following schema:\n",
    "`<label>,<integer feature 1>, ... <integer feature 13>,<categorical feature 1>, ... <categorical feature 26>,<id>`\n",
    "\n",
    "> When a value is missing, the field is \"\". There is no `label` field in the test set.\n",
    "\n",
    "</details>\n",
    "    \n",
    "## Metrics\n",
    "\n",
    "The evaluation metrics for this task are\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* [Normalized Entropy](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/sgd_logreg_nn/notebooks'\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin our introduction to Spark [MLlib](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset preprocessing\n",
    "\n",
    "Before we can train any prediction model on our dataset we need to conver each row into real-valued features vector ($\\vecx \\in \\mathbb{R}^n$).\n",
    "\n",
    "Spark MLlib provides easy to use tools for preprocessing raw features and turning them into suitable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(False, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only first two categorical features for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = ['_c{}'.format(i) for i in range(1, 14)]\n",
    "cat_columns = ['_c{}'.format(i) for i in range(14, 40)][:2]\n",
    "len(num_columns), len(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ML Pipelines (Transformers, Estimators)](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components)\n",
    "\n",
    "\n",
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n",
    "\n",
    "* `Transformer`: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "\n",
    "* `Estimator`: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "\n",
    "* `Pipeline`: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "---\n",
    "Basically speaking `transformer` is an instance of class that implements `transform` method, and both `estimator` and `pipeline` implements `transform` and `fit` methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, _c1=0, _c2=-1, _c3=0, _c4=0, _c5=1465, _c6=0, _c7=17, _c8=0, _c9=4, _c10=0, _c11=4, _c12=0, _c13=0, _c14='241546e0', _c15='38a947a1', _c16='fa673455', _c17='6a14f9b9', _c18='25c83c98', _c19='fe6b92e5', _c20='1c86e0eb', _c21='1f89b562', _c22='a73ee510', _c23='e7ba2569', _c24='755e4a50', _c25='208d9687', _c26='5978055e', _c27='07d13a8f', _c28='5182f694', _c29='f8b34416', _c30='e5ba7672', _c31='e5f8f18f', _c32=None, _c33=None, _c34='f3ddd519', _c35=None, _c36='32c7478e', _c37='b34f3128', _c38=None, _c39=None, id=12)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stages of pipeline\n",
    "\n",
    "We might benefit from using `StringIndexer, OneHotEncoderEstimator, VectorAssembler` (see [doc](https://spark.apache.org/docs/latest/ml-features) for details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "\n",
    "idx_columns = [col + \"_idx\" for col in cat_columns]\n",
    "encoded_columns = [col + \"_encoded\" for col in cat_columns]\n",
    "\n",
    "str_indexers = [StringIndexer(inputCol=col, outputCol=col + \"_idx\", handleInvalid='keep') for col in cat_columns]\n",
    "encoder = OneHotEncoderEstimator(inputCols=idx_columns, outputCols=encoded_columns)\n",
    "assembler = VectorAssembler(inputCols=num_columns + encoded_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=str_indexers + [encoder, assembler])\n",
    "\n",
    "fitted = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PATH = os.path.join(DATA_PATH, 'pipeline')\n",
    "\n",
    "fitted.write().overwrite().save(PIPELINE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fitted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipeline = PipelineModel.load(PIPELINE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=12, features=SparseVector(1950, {1: -1.0, 4: 1465.0, 6: 17.0, 8: 4.0, 10: 4.0, 25: 1.0, 1398: 1.0}), _c0=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pipeline.transform(df)\n",
    "new_df = new_df.select('id', 'features', '_c0')\n",
    "new_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset split\n",
    "\n",
    "Spark provides [randomSplit](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method.\n",
    "\n",
    "It is not the best choice in our task since we have chronological order in data.\n",
    "\n",
    "We need to implement our own split function which will split the data in parts with respect to chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def split_by_col(df, split_col, parts_fractions):\n",
    "    \"\"\"\n",
    "    df - DataFrame\n",
    "    split_col - total order column\n",
    "    parts_fractions - fractions of resulting parts\n",
    "    \"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    window = Window().orderBy(split_col)\n",
    "    df = df.withColumn('percent_rank', F.percent_rank().over(window))\n",
    "    \n",
    "    for idx, fraction in enumerate(parts_fractions):\n",
    "        part_values = df.filter(\n",
    "            (F.col('percent_rank') > sum(parts_fractions[:idx])) &\n",
    "            (F.col('percent_rank') <= sum(parts_fractions[:idx + 1]))).drop('percent_rank')\n",
    "        \n",
    "        parts.append(part_values)\n",
    "    \n",
    "    return parts\n",
    "\n",
    "train_df, val_df, test_df = split_by_col(new_df, 'id', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1832839"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = df.count()\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999993452780086, 0.10000005456016595, 0.10000005456016595)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() / N, val_df.count() / N, test_df.count() / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Classification](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "## [Logistic Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)\n",
    "\n",
    "### Define and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 13.4 ms, total: 40.2 ms\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "label_column = '_c0'\n",
    "\n",
    "lr_model = LogisticRegression(featuresCol='features', labelCol=label_column)\n",
    "lr_model = lr_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=455266594741, features=SparseVector(1950, {2: 58.0, 3: 20.0, 4: 2891.0, 5: 20.0, 6: 9.0, 7: 23.0, 8: 20.0, 10: 1.0, 12: 20.0, 39: 1.0, 1402: 1.0}), _c0=0, rawPrediction=DenseVector([1.8442, -1.8442]), probability=DenseVector([0.8634, 0.1366]), prediction=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.transform(val_df).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Evaluation](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)\n",
    "\n",
    "## [Binary classification metrics](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#binary-classification)\n",
    "\n",
    "* ROC AUC\n",
    "* LogLoss\n",
    "* Normalized Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "def rocauc(model, df):\n",
    "    predicts = model.transform(df).rdd.map(lambda row: (float(row['probability'][1]), float(row[label_column])))\n",
    "    return BinaryClassificationMetrics(predicts).areaUnderROC\n",
    "\n",
    "\n",
    "def compute_loss(row):\n",
    "    proba, label = row\n",
    "    return label * np.log(proba) + (1 - label) * np.log(1 - proba)\n",
    "\n",
    "\n",
    "def logloss(df):\n",
    "    return df.map(lambda row: compute_loss(row)).mean()\n",
    "\n",
    "\n",
    "def ne(model, df):\n",
    "    predicts = model.transform(df).rdd.map(lambda row: (float(row['probability'][1]), float(row[label_column])))\n",
    "    \n",
    "    l1_loss = logloss(predicts)\n",
    "    avg_label = predicts.map(lambda row: row[1]).mean()\n",
    "    \n",
    "    return l1_loss / compute_loss([avg_label, avg_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030084715755153\n",
      "0.9236270891570331\n",
      "0.7000710053901746\n",
      "0.9262272701785131\n",
      "CPU times: user 97.6 ms, sys: 13.4 ms, total: 111 ms\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# my metrics\n",
    "\n",
    "print(rocauc(lr_model, val_df))\n",
    "print(ne(lr_model, val_df))\n",
    "print(rocauc(lr_model, test_df))\n",
    "print(ne(lr_model, test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030084715755153"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236270891570331"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000710053901746"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262272701785131"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne(lr_model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission\n",
    "\n",
    "Join the [competition](https://www.kaggle.com/c/mlbd-20-ctr-prediction-1) and make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, is_train=False):\n",
    "    df = df.fillna(0, subset=num_columns)\n",
    "    df = pipeline.transform(df)\n",
    "    \n",
    "    selected_features = ['id', 'features']\n",
    "    if is_train:\n",
    "        selected_features += [label_column]\n",
    "    \n",
    "    return df.select(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = os.path.join(DATA_PATH, 'test.csv')\n",
    "\n",
    "submit_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_df(df, is_train=True)\n",
    "submit_df = process_df(submit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = processed_df.sample(False, 0.5)\n",
    "train, test = split_by_col(sampled, 'id', [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.25, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.5, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 0.75, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0.2, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 10}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 50}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.01, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.1, 'maxIter': 100}\n",
      "{'elasticNetParam': 1, 'fitIntercept': False, 'regParam': 0.2, 'maxIter': 100}\n",
      "CPU times: user 4 s, sys: 747 ms, total: 4.75 s\n",
      "Wall time: 1h 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'elasticNetParam': [0, 0.25, 0.5, 0.75, 1], \n",
    "    'fitIntercept': [True, False],\n",
    "    'maxIter': [10, 50, 100],\n",
    "    'regParam': [0, 0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "max_auc = -1\n",
    "saved_model = None\n",
    "max_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    lr_model = LogisticRegression(featuresCol='features', labelCol=label_column, **params)\n",
    "    lr_model = lr_model.fit(train)\n",
    "    \n",
    "    cur_auc = rocauc(lr_model, test)\n",
    "    \n",
    "    if max_auc <= cur_auc:\n",
    "        max_auc = cur_auc\n",
    "        saved_model = lr_model\n",
    "        max_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'elasticNetParam': 1, 'fitIntercept': True, 'regParam': 0, 'maxIter': 10}\n",
      "0.7026011473055717\n"
     ]
    }
   ],
   "source": [
    "print(max_params)\n",
    "print(max_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(featuresCol='features', labelCol=label_column, **max_params)\n",
    "lr_model = lr_model.fit(sampled)\n",
    "\n",
    "saved_model = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 211 ms, total: 20.9 s\n",
      "Wall time: 23.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566935904713</td>\n",
       "      <td>[0.7438637807325406, 0.25613621926745944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>566935904715</td>\n",
       "      <td>[0.7106944340329149, 0.28930556596708507]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>566935904727</td>\n",
       "      <td>[0.689059261600075, 0.310940738399925]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>566935904737</td>\n",
       "      <td>[0.48403990617722636, 0.5159600938227736]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>566935904741</td>\n",
       "      <td>[0.711774128524775, 0.2882258714752251]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                probability\n",
       "0  566935904713  [0.7438637807325406, 0.25613621926745944]\n",
       "1  566935904715  [0.7106944340329149, 0.28930556596708507]\n",
       "2  566935904727     [0.689059261600075, 0.310940738399925]\n",
       "3  566935904737  [0.48403990617722636, 0.5159600938227736]\n",
       "4  566935904741    [0.711774128524775, 0.2882258714752251]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predicted_submit = saved_model.transform(submit_df).select('id', 'probability').toPandas()\n",
    "predicted_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_submit['probability'] = predicted_submit['probability'] \\\n",
    "            .map(lambda probas: probas[-1] if not np.isnan(probas[-1]) else 0)\n",
    "\n",
    "predicted_submit = predicted_submit.rename(columns={'probability': 'proba'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_PATH = os.path.join(DATA_PATH, 'submit.csv')\n",
    "predicted_submit.to_csv(SUBMIT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
